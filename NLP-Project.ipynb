{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10011671,"sourceType":"datasetVersion","datasetId":6163632},{"sourceId":10017134,"sourceType":"datasetVersion","datasetId":6167658}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pypdf\n!pip install -U sentence-transformers\n!pip install pinecone","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T08:56:23.460853Z","iopub.execute_input":"2024-11-26T08:56:23.461572Z","iopub.status.idle":"2024-11-26T08:56:49.010030Z","shell.execute_reply.started":"2024-11-26T08:56:23.461529Z","shell.execute_reply":"2024-11-26T08:56:49.008846Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: pypdf in /opt/conda/lib/python3.10/site-packages (5.0.1)\nRequirement already satisfied: typing_extensions>=4.0 in /opt/conda/lib/python3.10/site-packages (from pypdf) (4.12.2)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (3.3.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: pinecone in /opt/conda/lib/python3.10/site-packages (5.4.0)\nRequirement already satisfied: certifi>=2019.11.17 in /opt/conda/lib/python3.10/site-packages (from pinecone) (2024.8.30)\nRequirement already satisfied: pinecone-plugin-inference<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pinecone) (2.0.1)\nRequirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /opt/conda/lib/python3.10/site-packages (from pinecone) (0.0.7)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from pinecone) (2.9.0.post0)\nRequirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from pinecone) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from pinecone) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from pinecone) (1.26.18)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone) (1.16.0)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom pypdf import PdfReader\nimport re\nimport numpy as np\nfrom numpy.linalg import norm","metadata":{"_uuid":"4d7c60e2-a483-4348-b2f7-478f22b41497","_cell_guid":"26bd4e3d-7e83-481d-ae12-3b90a46ca9fd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-11-26T08:56:49.012587Z","iopub.execute_input":"2024-11-26T08:56:49.013002Z","iopub.status.idle":"2024-11-26T08:56:49.018344Z","shell.execute_reply.started":"2024-11-26T08:56:49.012955Z","shell.execute_reply":"2024-11-26T08:56:49.017347Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"**Embedding Using Transformer**","metadata":{}},{"cell_type":"code","source":"pip install -U sentence-transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T08:56:49.019738Z","iopub.execute_input":"2024-11-26T08:56:49.020183Z","iopub.status.idle":"2024-11-26T08:56:57.501436Z","shell.execute_reply.started":"2024-11-26T08:56:49.020143Z","shell.execute_reply":"2024-11-26T08:56:57.500313Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (3.3.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"**Custom Embedding Using Skills Extraction from CV**","metadata":{}},{"cell_type":"code","source":"import re\n\ndef c(text):\n    a = text.count(\"c/c++\")\n    return len(re.findall(r\"(\\t|\\n|,|\\.| |^)c( |$|\\t|\\n|,|\\.)\", text)) + a\n\ndef cpp(text):\n    a = len(re.findall(r\"(\\t|\\n|,|\\.| |^)cpp( |$|\\t|\\n|,|\\.)\", text))\n    b = len(re.findall(r\"(\\t|\\n|,|\\.| |^)c\\+\\+( |$|\\t|\\n|,|\\.)\", text))\n    c = text.count(\"c/c++\")\n    return a + b + c\n\ndef assembly(text):\n    return text.count(\"assembly\")\n\ndef java(text):\n    return len(re.findall(r\"(\\t|,|\\.|\\n| |^)java( |$|\\t|,|\\.|\\n)\", text))\n\ndef js(text):\n    return len(re.findall(r\"(\\t|,|\\.|\\n| |^)js( |$|\\t|,|\\.|\\n)\", text))\n\ndef objC(text):\n    return text.count(\"objective c\") + text.count(\"objective-c\")\n\ndef datastructs(text):\n    return sum(text.count(term) for term in [\"datastructures\", \"data-structures\", \"data structures\"])\n\ndef socket(text):\n    return len(re.findall(r\"(\\t|,|\\.|\\n| |^)socket( |$|\\t|\\n|,|\\.)\", text))\n\ndef sql(text):\n    return len(re.findall(r\"(\\t|\\n|,|\\.| |^)sql( |$|\\t|,|\\.|\\n)\", text))\n\ndef go(text):\n    return len(re.findall(r\"(\\t|\\n|,|\\.| |^)go( |$|\\t|\\n|,|\\.)\", text))\n\ndef opencv(text):\n    return text.count(\"opencv\")\n\ndef boost(text):\n    return text.count(\"boost\")\n\ndef eigen(text):\n    return text.count(\"eigen\")\n\ndef numpy(text):\n    return text.count(\"numpy\")\n\ndef scikit_learn(text):\n    return len(re.findall(r\"scikit-learn\", text, re.IGNORECASE))\n\ndef keras(text):\n    return text.count(\"keras\")\n\ndef matplotlib(text):\n    return text.count(\"matplotlib\")\n\ndef seaborn(text):\n    return text.count(\"seaborn\")\n\ndef nodejs(text):\n    return len(re.findall(r\"(node\\.js|nodejs)\", text, re.IGNORECASE))\n\ndef expressjs(text):\n    return len(re.findall(r\"(express\\.js|expressjs)\", text, re.IGNORECASE))\n\ndef jquery(text):\n    return len(re.findall(r\"(jquery|jQuery)\", text))\n\ndef unity(text):\n    return text.count(\"unity\")\n\ndef unreal_engine(text):\n    return len(re.findall(r\"(unreal engine|unrealengine)\", text, re.IGNORECASE))\n\ndef pygame(text):\n    return text.count(\"pygame\")\n\n\ndims = {\n    \"c\": 0,\n    \"cpp\": 1,\n    \"assembly\": 2,\n    \"python\": 3,\n    \"java\": 4,\n    \"javascript\": 5,\n    \"html\": 6,\n    \"css\": 7,\n    \"lua\": 8,\n    \"ruby\": 9,\n    \"rust\": 10,\n    \"go\": 11,\n    \"swift\": 12,\n    \"objective-c\": 13,\n    \"android development\": 14,\n    \"ios development\": 15,\n    \"mobile development\": 16,\n    \"web development\": 17,\n    \"machine learning\": 18,\n    \"data science\": 19,\n    \"embedded development\": 20,\n    \"data structures\": 21,\n    \"tensorflow\": 22,\n    \"pytorch\": 23,\n    \"pandas\": 24,\n    \"reactjs\": 25,\n    \"nextjs\": 26,\n    \"vue\": 27,\n    \"angular\": 28,\n    \"bootstrap\": 29,\n    \"sdl\": 30,\n    \"flex\": 31,\n    \"bison\": 32,\n    \"llvm\": 33,\n    \"compiler\": 34,\n    \"libcurl\": 35,\n    \"websockets\": 36,\n    \"socket\": 37,\n    \"springboot\": 38,\n    \"laravel\": 39,\n    \"django\": 40,\n    \"fastapi\": 41,\n    \"flask\": 42,\n    \"sql\": 43,\n    \"mongodb\": 44,\n    \"mysql\": 45,\n    \"postgresql\": 46,\n    \"mssql\": 47,\n    \"qt\": 48,\n    \"php\": 49,\n    \"opencv\": 50,\n    \"boost\": 51,\n    \"eigen\": 52,\n    \"numpy\": 53,\n    \"scikit-learn\": 54,\n    \"keras\": 55,\n    \"matplotlib\": 56,\n    \"seaborn\": 57,\n    \"node.js\": 58,\n    \"express.js\": 59,\n    \"jquery\": 60,\n    \"unity\": 61,\n    \"unreal engine\": 62,\n    \"pygame\": 63,\n    \"gtk\": 64,\n    \"glib\": 65,\n    \"openssl\": 66,\n    \"ncurses\": 67,\n    \"tailwindcss\": 68,\n    \"bulma\": 69,\n    \"foundation\": 70,\n    \"svelte\": 71,\n    \"nuxtjs\": 72,\n    \"xgboost\": 73,\n    \"lightgbm\": 74,\n    \"flutter\": 75,\n    \"react native\": 76,\n    \"kotlin\": 77,\n    \"gcc\": 78,\n    \"clang\": 79,\n    \"yacc\": 80,\n    \"hibernate\": 81,\n    \"junit\": 82,\n    \"maven\": 83,\n    \"gradle\": 84,\n    \"kafka\": 85,\n    \"sqlite\": 86,\n    \"oracle db\": 87,\n    \"aws\": 88,\n    \"azure\": 89,\n    \"gcp\": 90,\n    \"docker\": 91,\n    \"kubernetes\": 92,\n    \"terraform\": 93,\n    \"sass\": 94,\n    \"less\": 95,\n    \"parcel\": 96,\n    \"vite\": 97,\n    \"frontend\": 98,\n    \"backend\": 99,\n    \"general programming\": 100,\n    \"ruby on rails\": 101,\n    \"scipy\": 102,\n    \"beautifulsoup\": 103,\n    \"scrapy\": 104,\n    \"three.js\": 105,  \n    \"d3.js\": 106,\n    \"cloud\":107,\n}\n\ndef generate_embedding(text):\n    return [\n        c(text),\n        cpp(text),\n        text.count(\"assembly\"),\n        text.count(\"python\"),\n        java(text),\n        text.count(\"javascript\") + js(text),\n        text.count(\"html\"),\n        text.count(\"css\"),\n        text.count(\"lua\"),\n        text.count(\"ruby\"),\n        text.count(\"rust\"),\n        go(text),\n        text.count(\"swift\"),\n        objC(text),\n        text.count(\"android\"),\n        text.count(\"ios\"),\n        text.count(\"mobile development\"),\n        text.count(\"web\"),\n        text.count(\"machine learning\"),\n        text.count(\"data science\"),\n        text.count(\"embedded development\"),\n        datastructs(text),\n        text.count(\"tensorflow\"),\n        text.count(\"pytorch\"),\n        text.count(\"pandas\"),\n        text.count(\"reactjs\"),\n        text.count(\"nextjs\"),\n        text.count(\"vue\"),\n        text.count(\"angular\"),\n        text.count(\"bootstrap\"),\n        text.count(\"sdl\"),\n        text.count(\"flex\"),\n        text.count(\"bison\"),\n        text.count(\"llvm\"),\n        text.count(\"compiler\"),\n        text.count(\"libcurl\"),\n        text.count(\"websocket\"),\n        socket(text),\n        text.count(\"springboot\"),\n        text.count(\"laravel\"),\n        text.count(\"django\"),\n        text.count(\"fastapi\"),\n        text.count(\"flask\"),\n        sql(text),\n        text.count(\"mongodb\"),\n        text.count(\"mysql\"),\n        text.count(\"postgresql\"),\n        text.count(\"mssql\"),\n        text.count(\"qt\"),\n        text.count(\"php\"),\n        opencv(text),\n        boost(text),\n        eigen(text),\n        numpy(text),\n        scikit_learn(text),\n        keras(text),\n        matplotlib(text),\n        seaborn(text),\n        nodejs(text),\n        expressjs(text),\n        jquery(text),\n        text.count(\"unity\"),\n        unreal_engine(text),\n        pygame(text),\n        text.count(\"gtk\"),\n        text.count(\"glib\"),\n        text.count(\"openssl\"),\n        text.count(\"ncurses\"),\n        text.count(\"tailwindcss\"),\n        text.count(\"bulma\"),\n        text.count(\"foundation\"),\n        text.count(\"svelte\"),\n        text.count(\"nuxtjs\"),\n        text.count(\"xgboost\"),\n        text.count(\"lightgbm\"),\n        text.count(\"flutter\"),\n        text.count(\"react native\"),\n        text.count(\"kotlin\"),\n        text.count(\"gcc\"),\n        text.count(\"clang\"),\n        text.count(\"yacc\"),\n        text.count(\"hibernate\"),\n        text.count(\"junit\"),\n        text.count(\"maven\"),\n        text.count(\"gradle\"),\n        text.count(\"kafka\"),\n        text.count(\"sqlite\"),\n        text.count(\"oracle db\"),\n        text.count(\"aws\"),\n        text.count(\"azure\"),\n        text.count(\"gcp\"),\n        text.count(\"docker\"),\n        text.count(\"kubernetes\"),\n        text.count(\"terraform\"),\n        text.count(\"sass\"),\n        text.count(\"less\"),\n        text.count(\"parcel\"),\n        text.count(\"vite\"),\n        text.count(\"frontend\"),\n        text.count(\"backend\"),\n        text.count(\"general programming\"),\n        text.count(\"ruby on rails\"),\n        text.count(\"beautifulsoup\"),\n        text.count(\"scipy\"),\n        text.count(\"scrapy\"),\n        text.count(\"three.js\"),\n        text.count(\"d3.js\"),\n        text.count(\"cloud\")\n    ]\n\n\ndef add_weighted_sum(vec,to_update,keys,weights):\n    i = 0\n    for key in keys:\n        vec[dims[to_update]] += vec[dims[key]] * weights[i]\n        i += 1\ndef add_dep_score(vec):\n    # SDL, Flex, Bison are C libraries\n    add_weighted_sum(vec, \"c\", [\"sdl\", \"flex\", \"bison\", \"gtk\", \"glib\", \"libcurl\", \"openssl\", \"ncurses\"], [0.1] * 8)\n    # C influences C++\n    vec[dims[\"cpp\"]] += 0.4 * vec[dims[\"c\"]]\n    # TensorFlow, PyTorch, and other ML libraries improve machine learning skills\n    add_weighted_sum(vec, \"machine learning\", [\"pytorch\", \"tensorflow\", \"scikit-learn\", \"keras\", \"xgboost\", \"lightgbm\"], [0.5] * 6)\n    # Bootstrap is CSS\n    add_weighted_sum(vec, \"css\", [\"bootstrap\", \"tailwindcss\", \"bulma\", \"foundation\"], [0.1] * 4)\n    add_weighted_sum(\n        vec, \n        \"web development\", \n        [\n            \"django\", \"fastapi\", \"laravel\", \"flask\", \"reactjs\", \"vue\", \"angular\", \"svelte\", \n            \"nextjs\", \"nuxtjs\", \"express.js\", \"ruby on rails\", \"html\", \"css\", \"javascript\"\n        ], \n        [0.2] * 15\n    )\n    add_weighted_sum(\n        vec, \n        \"python\", \n        [\"django\", \"flask\", \"pytorch\", \"tensorflow\", \"pandas\", \"numpy\", \"scipy\", \"matplotlib\", \"seaborn\", \"fastapi\"], \n        [0.1] * 10\n    )\n    add_weighted_sum(\n        vec, \n        \"javascript\", \n        [\"reactjs\", \"vue\", \"angular\", \"nextjs\", \"nuxtjs\", \"jquery\", \"d3.js\", \"three.js\", \"svelte\"], \n        [0.1] * 9\n    )\n    add_weighted_sum(vec, \"mobile development\", [\"android development\", \"ios development\", \"flutter\", \"react native\", \"kotlin\"], [0.5] * 5)\n    # Flex, Bison, LLVM improve compiler construction skills\n    add_weighted_sum(vec, \"compiler\", [\"llvm\", \"flex\", \"bison\", \"gcc\", \"clang\", \"yacc\"], [0.7] * 6)\n    # Qt improves C++ skills\n    add_weighted_sum(vec, \"cpp\", [\"qt\", \"boost\", \"opencv\", \"eigen\"], [0.3] * 4)\n    # Java frameworks and libraries\n    add_weighted_sum(vec, \"java\", [\"springboot\", \"hibernate\", \"junit\", \"maven\", \"gradle\", \"kafka\"], [0.1] * 6)\n    # SQL tools and databases\n    add_weighted_sum(vec, \"sql\", [\"mysql\", \"mssql\", \"postgresql\", \"sqlite\", \"mongodb\", \"oracle db\"], [0.2] * 6)\n    # Cloud tools and DevOps influence skills in these areas\n    add_weighted_sum(vec, \"cloud\", [\"aws\", \"azure\", \"gcp\", \"docker\", \"kubernetes\", \"terraform\"], [0.3] * 6)\n    # Frontend development includes additional CSS and JS libraries\n    add_weighted_sum(vec, \"frontend\", [\"tailwindcss\", \"sass\", \"less\", \"bootstrap\", \"foundation\", \"parcel\", \"vite\"], [0.2] * 7)\n    # Backend development includes Node.js, Express, and others\n    add_weighted_sum(vec, \"backend\", [\"express.js\", \"fastapi\", \"flask\", \"django\", \"laravel\", \"springboot\"], [0.2] * 6)\n    # Additional general-purpose libraries for programming\n    add_weighted_sum(vec, \"general programming\", [\"numpy\", \"pandas\", \"matplotlib\", \"scipy\", \"beautifulsoup\", \"scrapy\"], [0.2] * 6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T08:56:57.503780Z","iopub.execute_input":"2024-11-26T08:56:57.504309Z","iopub.status.idle":"2024-11-26T08:56:57.554006Z","shell.execute_reply.started":"2024-11-26T08:56:57.504240Z","shell.execute_reply":"2024-11-26T08:56:57.552881Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"**JOB DESCRIPTION PART**","metadata":{}},{"cell_type":"markdown","source":"**JOB DESCRIPTION DATABASE CREATION**","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from tqdm import tqdm\n# from sentence_transformers import SentenceTransformer\n\n# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\n# # Define a function to split text into chunks\n# def split_text_into_chunks(text, max_tokens=512):\n#     \"\"\"\n#     Split the text into chunks of max_tokens, handling tokenization limits.\n#     \"\"\"\n#     tokens = model.tokenizer.encode(text)  # Tokenize text\n#     chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n#     return chunks\n\n# # Define a function to generate embeddings for a single text\n# def generate_transformer_embedding(text):\n#     \"\"\"\n#     Generate the average embedding for a long text using all-MiniLM-L6-v2.\n#     \"\"\"\n#     # Split the text into manageable chunks\n#     chunks = split_text_into_chunks(text, max_tokens=512)\n    \n#     # Generate embeddings for each chunk\n#     embeddings_jd_arr = []\n#     for chunk in chunks:\n#         # Decode chunk tokens back to text (for batch processing) and generate embedding\n#         chunk_text = model.tokenizer.decode(chunk)\n#         chunk_embedding = model.encode(chunk_text)\n#         embeddings_jd_arr.append(chunk_embedding)\n\n#     # Convert to numpy array\n#     embeddings_jd_arr = np.array(embeddings_jd_arr)\n    \n#     # Average the embeddings to get a single representation\n#     combined_jd_embedding = embeddings_jd_arr.mean(axis=0)\n    \n#     return combined_jd_embedding\n\n# def generate_custom_embedding(jd):\n#     jd_vec = generate_embedding(jd)\n#     for i in range(50):\n#         if jd_vec[i] != 0:\n#             print(ivd[i])\n#     add_dep_score(jd_vec)\n#     return jd_vec\n\n# # Define a function to generate embeddings for the DataFrame\n# def process_dataframe_embeddings(df):\n#     \"\"\"\n#     Process the DataFrame to generate embeddings for each row and store the combined embeddings.\n#     \"\"\"\n#     all_job_embeddings = []\n    \n#     for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n#         # Combine the text fields (query + job + description)\n#         combined_text = row['Query'] + \" \" + row['Job Title'] + \" \" + row['Description']\n        \n#         # Generate the transformer embedding\n#         job_transformer_embedding = generate_transformer_embedding(combined_text)\n        \n#         # Generate the custom embedding\n#         job_custom_embedding = np.array(generate_custom_embedding(combined_text))  # Use your custom logic\n        \n#         # Concatenate both embeddings\n#         weight=0.5\n#         jd_embedding = np.concatenate(\n#                 [weight * np.array(job_custom_embedding), (1 - weight) * np.array(job_transformer_embedding)]\n#             )\n        \n#         # Append the final embedding\n#         all_job_embeddings.append(jd_embedding)\n    \n#     return np.array(all_job_embeddings)\n\n# df_job = pd.read_csv(\"/kaggle/input/jobs-description-dataset/JobsDataset.csv\")\n# df_job['Query'] = df_job['Query'].str.lower()\n# df_job['Job Title'] = df_job['Job Title'].str.lower()\n# df_job['Description'] = df_job['Description'].str.lower()\n\n# df_job_embeddings = process_dataframe_embeddings(df_job)\n# df_job['Embedding'] = list(df_job_embeddings)\n\n# # Save or inspect the embeddings\n# print(f\"Generated embeddings shape: {df_job_embeddings.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T08:56:57.557045Z","iopub.execute_input":"2024-11-26T08:56:57.557470Z","iopub.status.idle":"2024-11-26T08:56:57.567410Z","shell.execute_reply.started":"2024-11-26T08:56:57.557425Z","shell.execute_reply":"2024-11-26T08:56:57.566192Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# # Save the DataFrame to a new CSV file\n# df_job.to_csv(\"jobs_with_embeddings.csv\", index=False)\n\n# print(\"DataFrame with embeddings has been saved to 'jobs_with_embeddings.csv'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T08:56:57.568630Z","iopub.execute_input":"2024-11-26T08:56:57.568987Z","iopub.status.idle":"2024-11-26T08:56:57.582428Z","shell.execute_reply.started":"2024-11-26T08:56:57.568939Z","shell.execute_reply":"2024-11-26T08:56:57.581301Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# df_job['Embedding'][0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T08:56:57.583937Z","iopub.execute_input":"2024-11-26T08:56:57.584333Z","iopub.status.idle":"2024-11-26T08:56:57.594537Z","shell.execute_reply.started":"2024-11-26T08:56:57.584258Z","shell.execute_reply":"2024-11-26T08:56:57.593778Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"**PINECONE**","metadata":{}},{"cell_type":"code","source":"# import os\n# from pinecone import Pinecone, ServerlessSpec\n# import numpy as np\n# import math\n\n# # Initialize Pinecone with your API key\n# pc = Pinecone(\n#     api_key=\"pcsk_2BVuyU_8HtemHZ7ZA2CQGFpbhUePfNfcu4pZA9RgKrGvYPYge7qWA9v9QsUFMGt9UW3xLs\"  # Replace with your Pinecone API key\n# )\n\n# # Create the index if it doesn't exist\n# index_name = \"job-embedding\"\n\n# # Check if the index already exists\n# if index_name not in pc.list_indexes().names():\n#     pc.create_index(\n#         name=index_name,\n#         dimension=492,  # Change this to the correct embedding dimension (e.g., 384 for MiniLM)\n#         metric=\"cosine\",  # You can also use \"euclidean\" or other distance metrics\n#         spec=ServerlessSpec(\n#             cloud=\"aws\",\n#             region=\"us-east-1\"  # You can modify the cloud and region settings as needed\n#         )\n#     )\n\n# # Connect to the index\n# index = pc.Index(index_name)\n\n\n# # Function to upload vectors in batches to Pinecone\n# def upload_vectors_in_batches(index, vectors, batch_size=100):\n#     # Split vectors into smaller batches\n#     num_batches = math.ceil(len(vectors) / batch_size)\n    \n#     for batch_idx in range(num_batches):\n#         # Slice the batch\n#         batch = vectors[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n        \n#         # Upload the batch to Pinecone\n#         try:\n#             index.upsert(batch)\n#             print(f\"Uploaded batch {batch_idx + 1}/{num_batches} of size {len(batch)}\")\n#         except Exception as e:\n#             print(f\"Error uploading batch {batch_idx + 1}: {e}\")\n        \n# # Prepare vectors for upload\n# vectors = []\n# for i, row in df_job.iterrows():\n#     # Check if the embedding is None or NaN\n#     if row['Embedding'] is None or (isinstance(row['Embedding'], (np.ndarray, list)) and np.any(np.isnan(row['Embedding']))):\n#         print(f\"Skipping row {i} due to missing or invalid embedding.\")\n#         continue  # Skip this row if embedding is missing or contains NaN\n    \n#     # Convert embeddings to a list (if numpy array or similar)\n#     vector = row['Embedding'] if isinstance(row['Embedding'], list) else row['Embedding'].tolist()\n    \n#     # Prepare metadata (description and wrvu), set default wrvu if missing\n#     metadata = {\n#         \"query\": row['Query'],\n#         \"job\": row['Job Title'],\n#         \"description\": row['Description'],\n#     }\n\n#     # Create a vector dictionary to upload to Pinecone\n#     vectors.append({\n#         'id': f\"job_{i}\",  \n#         'values': vector,        # Embedding values\n#         'metadata': metadata     # Optional metadata (description, wrvu)\n#     })\n\n# # Upload the vectors in batches\n# upload_vectors_in_batches(index, vectors, batch_size=100)\n\n# # Confirmation\n# print(f\"Total vectors uploaded: {len(vectors)}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T08:56:57.595808Z","iopub.execute_input":"2024-11-26T08:56:57.596074Z","iopub.status.idle":"2024-11-26T08:56:57.602189Z","shell.execute_reply.started":"2024-11-26T08:56:57.596050Z","shell.execute_reply":"2024-11-26T08:56:57.601512Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"**TO BE ADDED IN A SINGLE FILE**","metadata":{}},{"cell_type":"markdown","source":"**Query From Pinecone Part**","metadata":{}},{"cell_type":"code","source":"reader = PdfReader('/kaggle/input/resumes/sadeem.pdf')\ntext = ''\nfor page in reader.pages:\n    text += '\\n' + page.extract_text()\ntext=text.lower()\nprint(text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T09:08:44.603639Z","iopub.execute_input":"2024-11-26T09:08:44.604259Z","iopub.status.idle":"2024-11-26T09:08:44.671210Z","shell.execute_reply.started":"2024-11-26T09:08:44.604223Z","shell.execute_reply":"2024-11-26T09:08:44.670504Z"}},"outputs":[{"name":"stdout","text":"\nprogramming \nc/c++/c#\npython\njs/ts\nhtml/csscreatives\ninkscape\nfigma/miro\nhitfilm\nadobe stackdev\nunity 2d\nsveltekit\nneo4j/mongo\nbootstrap\ntailwind/daisyproblem\nsolving\np. logic\nc. thinking\nphilosophysadeem sajid\nrelentless problem solving\nabout me education & courses\na thorough and systematic computer\nscience enthusiast proficient in devising\nprogram solutions, web dev, and\ncreatives. intern @auxcube.2016-2019: o-levels (ssc equivalent\nof science group), lahore college of\narts and sciences. 8a*/a + 1b\n2019-2021: a-levels (hssc equivalent\nof computer science group), lahore\ncollege of arts and sciences. 3a*\n2021-present: bs(cs), fast nuces\nlahore. cgpa: 3.87\nskills\n+92 333 4273308 • sadeem.sajid@outlook.comphilosophy of science. university of\npennsylvania. coursera.work experience\n&projects\n2022 sfml based oop game for course.\n2021 website for ict project. (sadeemsajid.github.io/thegoldenage/home.html)\n2020 ranked 5/106 in kindred game jam on itch.io. (frostblood.itch.io/decay-paper)\n2019 intern at hashtag tribe, a digital marketing agency.\n2019 incubated at plan9 for a start-up named sukoon.ai for 6 months.\n(instagram.com/sukoon.ai). served as cto, co-founder.\n2019 president of media and publications society at a-levels college, as well as the\nchief graphic designer of the lacas chronicles, a student-run newspaper.2024 software engineering intern at\nauxcube.\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"**CV Transformer Embedding**","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport numpy as np\n\n# Load the pre-trained all-MiniLM-L6-v2 model\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\n# Tokenize the text into chunks (512 tokens max per chunk)\ndef split_text_into_chunks(text, max_tokens=512):\n    \"\"\"\n    Split the text into chunks of max_tokens, handling tokenization limits.\n    \"\"\"\n    tokenizer = model.tokenizer\n    tokens = tokenizer.encode(text)  # Tokenize text\n    chunks = []\n    \n    for i in range(0, len(tokens), max_tokens):\n        chunks.append(tokens[i:i + max_tokens])\n    \n    return chunks\n\n# Split the CV text into chunks\nchunks = split_text_into_chunks(text, max_tokens=512)\n\n# Generate embeddings for each chunk\ntransformer_cv_embeddings_arr = []\nfor chunk in chunks:\n    # Decode chunk tokens back to text (for batch processing) and generate embedding\n    chunk_text = model.tokenizer.decode(chunk)\n    chunk_embedding = model.encode(chunk_text)\n    transformer_cv_embeddings_arr.append(chunk_embedding)\n\n# Convert transformer_cv_embeddings_arr to a numpy array\ntransformer_cv_embeddings_arr = np.array(transformer_cv_embeddings_arr)\n\n# Average the embeddings (to get a single representation of the entire CV)\ntransformer_cv_embedding = transformer_cv_embeddings_arr.mean(axis=0)\n\n# Print the final combined embedding dimensions\nprint(f\"Final combined embedding dimensions: {transformer_cv_embedding.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T09:08:52.841957Z","iopub.execute_input":"2024-11-26T09:08:52.842329Z","iopub.status.idle":"2024-11-26T09:08:53.540207Z","shell.execute_reply.started":"2024-11-26T09:08:52.842269Z","shell.execute_reply":"2024-11-26T09:08:53.539416Z"}},"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (388 > 256). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8b1e0badf3a4a948e2c9be0b2f84e7c"}},"metadata":{}},{"name":"stdout","text":"Final combined embedding dimensions: (384,)\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"**CV Custom Embedding**","metadata":{}},{"cell_type":"code","source":"cv_vec = generate_embedding(text)\nivd = {v: k for k, v in dims.items()}\n#print(len(vec))\nfor i in range(50):\n    if cv_vec[i] != 0:\n        print(ivd[i])\nadd_dep_score(cv_vec)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T09:08:57.481492Z","iopub.execute_input":"2024-11-26T09:08:57.481830Z","iopub.status.idle":"2024-11-26T09:08:57.488300Z","shell.execute_reply.started":"2024-11-26T09:08:57.481804Z","shell.execute_reply":"2024-11-26T09:08:57.487416Z"}},"outputs":[{"name":"stdout","text":"c\ncpp\npython\nhtml\ncss\nweb development\nbootstrap\n","output_type":"stream"}],"execution_count":51},{"cell_type":"markdown","source":"**Combining Both Embeddings (Transformer + Custom)**","metadata":{}},{"cell_type":"code","source":"weight=0.5\ncv_embedding = np.concatenate(\n        [weight * np.array(cv_vec), (1 - weight) * np.array(transformer_cv_embedding)]\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T09:09:01.609527Z","iopub.execute_input":"2024-11-26T09:09:01.609868Z","iopub.status.idle":"2024-11-26T09:09:01.614672Z","shell.execute_reply.started":"2024-11-26T09:09:01.609839Z","shell.execute_reply":"2024-11-26T09:09:01.613573Z"}},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":"**Query From Pinecone**","metadata":{}},{"cell_type":"code","source":"import os\nfrom pinecone import Pinecone\n\n# Set the API key\nos.environ[\"PINECONE_API_KEY\"] = \"pcsk_2BVuyU_8HtemHZ7ZA2CQGFpbhUePfNfcu4pZA9RgKrGvYPYge7qWA9v9QsUFMGt9UW3xLs\"  # Pinecone API key\n\n# Initialize Pinecone client\npc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n\n# Connect to Pinecone index\nindex_name = \"job-embedding\"  # Replace with your Pinecone index name\nindex = pc.Index(index_name)\n\n# Function to query Pinecone for similar items based on the given embedding\ndef query_pinecone(precomputed_embedding, index, top_k=5, similarity_threshold=0.5):\n    try:\n        query_embedding_list = precomputed_embedding.tolist()  # Ensure it's a list\n        query_results = index.query(\n            vector=query_embedding_list,  # The query vector as a list\n            top_k=top_k,                  # Number of closest matches to return\n            include_metadata=True         # Include metadata in the results\n        )\n\n        matches = []\n        for match in query_results['matches']:\n            score = match.get('score', 0)  # Get similarity score\n            if score >= similarity_threshold:\n                matches.append(match)\n        \n        return matches\n    except Exception as e:\n        print(f\"Error during Pinecone query: {e}\")\n        return []\n\n\n# Query Pinecone for results\ntop_k_results = query_pinecone(cv_embedding, index, top_k=10)\n\n# Format and print the results\nprint(\"Query Results:\")\n\nif top_k_results:\n    for i, match in enumerate(top_k_results, start=1):\n        metadata = match.get('metadata', {})\n        id_ = match.get('id', 'N/A')\n        score = match.get('score', 0)\n\n        # Print results\n        print(f\"\\n{i}. Result {i}:\")\n        print(f\"■ ID: {id_}\")\n        print(f\"■ Metadata: {metadata['job']}\")\n        print(f\"■ Similarity Score: {score:.2f}\")\n        print(\"-\" * 50)\nelse:\n    print(\"No results found above the similarity threshold.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T09:09:25.227153Z","iopub.execute_input":"2024-11-26T09:09:25.227568Z","iopub.status.idle":"2024-11-26T09:09:25.438766Z","shell.execute_reply.started":"2024-11-26T09:09:25.227535Z","shell.execute_reply":"2024-11-26T09:09:25.437874Z"}},"outputs":[{"name":"stdout","text":"Query Results:\n\n1. Result 1:\n■ ID: job_8917\n■ Metadata: .net web developer- movimento\n■ Similarity Score: 0.86\n--------------------------------------------------\n\n2. Result 2:\n■ ID: job_9627\n■ Metadata:  principal architect\n■ Similarity Score: 0.83\n--------------------------------------------------\n\n3. Result 3:\n■ ID: job_830\n■ Metadata: student assistant - geograhic information systems (gis) maintenance division\n(los angeles)\n■ Similarity Score: 0.80\n--------------------------------------------------\n\n4. Result 4:\n■ ID: job_5756\n■ Metadata:  software engineer, big data\n■ Similarity Score: 0.75\n--------------------------------------------------\n\n5. Result 5:\n■ ID: job_8865\n■ Metadata:  web developer ii\n■ Similarity Score: 0.73\n--------------------------------------------------\n\n6. Result 6:\n■ ID: job_9351\n■ Metadata:  security analyst\n■ Similarity Score: 0.72\n--------------------------------------------------\n\n7. Result 7:\n■ ID: job_8694\n■ Metadata:  full stack web developer\n■ Similarity Score: 0.72\n--------------------------------------------------\n\n8. Result 8:\n■ ID: job_5833\n■ Metadata:  data services developer\n■ Similarity Score: 0.71\n--------------------------------------------------\n\n9. Result 9:\n■ ID: job_5061\n■ Metadata:  web designer\n■ Similarity Score: 0.71\n--------------------------------------------------\n\n10. Result 10:\n■ ID: job_8742\n■ Metadata: full stack software engineer\n■ Similarity Score: 0.70\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}